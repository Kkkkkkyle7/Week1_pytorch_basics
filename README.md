# Week1_PyTorch_Basics - Week Summary（2026.2.9-2.15）

---

## 🏔️ 项目结构
- Day1_Tensor.ipynb：张量操作验证
- Day2_Autograd.ipynb：梯度计算直觉
- Day3_Dataset.ipynb：数据处理流程
- Day4_Model.ipynb：模型骨架搭建
- Day5_Loss.ipynb：损失函数认识
- Day6_Optimizer.ipynb：优化调参手感

---

## 🎯 本周目标
> 掌握深度学习工程核心流程：**基础操作 → 梯度计算 → 数据处理 → 模型设计 → 损失验证 → 优化器调参**，并用**数据验证**替代主观判断。

---

## 🔑 关键成果

| 任务 | 核心发现 | 量化结果 | 简历亮点 |
|------|----------|----------|----------|
| **数据预处理** | 30+特征工程增强特征表达力 | 31个特征（30个交互特征+1个原始特征） | "通过特征工程提升特征维度，为模型提供更丰富信息" |
| **Dataset实现** | `__len__`和`__getitem__`是数据处理基石 | `TitanicDataset(samples=637, features=31)` | "实现标准Dataset接口，确保数据加载可复现" |
| **`__repr__`设计** | 数据集状态需**一眼可读** | 输出：`TitanicDataset(samples=637, features=31)` | "通过`__repr__`让数据集'会说话'，提升调试效率" |
| **模型参数统计** | 模型参数=特征数+1（偏置项） | `Total parameters: 32` | "精准计算参数量（31+1=32），避免面试时模糊回答" |
| **损失函数维度** | `BCEWithLogitsLoss`要求`[batch]`输入 | 修复后`logits.squeeze(1)` | "解决PyTorch常见维度错误，确保训练稳定" |
| **优化器对比** | **小数据集需大学习率**（Adam lr=0.01） | Adam前2轮损失下降**快30%** | "实验证明Adam在小数据集收敛更快（0.1487 vs 0.1055）" |

---

## 💡 深度理解

### ✅ 为什么这个复盘是"简历级"的？
| 面试问题 | 回答（基于本周数据） | 为什么专业 |
|----------|--------------------------|------------|
| "你如何验证模型参数？" | "Day4中通过`sum(p.numel() for p in model.parameters())`计算出32个参数（31特征权重+1偏置），与Titanic特征数一致。" | ✅ 用代码+数据说话，不是死记硬背 |
| "为什么Adam比SGD收敛快？" | "在Titanic小数据集（637样本）上，Adam (lr=0.01) 前2轮损失下降0.1487，SGD (lr=0.1) 仅0.1055，**收敛速度快30%**。这验证了小数据集需大学习率（0.01）的工程原则。" | ✅ 量化结论+数据集规模关联 |
| "如何处理维度不匹配？" | "Day5中发现`logits`形状为[batch,1]，而损失函数要求[batch]，通过`squeeze(1)`修正。这避免了`ValueError`，是PyTorch训练的必修课。" | ✅ 问题定位+解决方案+工程价值 |

---

## 🌟 本周核心收获

> **"通过Titanic数据集全流程实践，掌握深度学习工程核心能力：**  
> 1. **数据处理**：实现标准化+特征工程，从637样本构建31特征数据集  
> 2. **模型设计**：验证模型参数量（32个）与特征数的理论关系  
> 3. **损失函数**：修复维度匹配问题，确保`BCEWithLogitsLoss`正确计算  
> 4. **优化器调参**：实验证明小数据集上Adam (lr=0.01) 比SGD (lr=0.1) 收敛快30%"
